# Flask Configuration
FLASK_APP=run.py
FLASK_ENV=development

# Database Configuration
# Format: postgresql://username:password@host:port/database_name
DATABASE_URL=postgresql://postgres:YOUR_PASSWORD_HERE@localhost:5432/hireskill_db

# JWT Configuration
JWT_SECRET_KEY=your-secret-key-change-this-in-production
JWT_ACCESS_TOKEN_EXPIRES=3600
JWT_REFRESH_TOKEN_EXPIRES=2592000

# ===== LLM Configuration for Resume & Cover Letter Generation =====
# Choose one of the options below:

# Option 1: Use Groq API (Fast, Cloud-based - Recommended for production)
LLM_API_URL=https://api.groq.com/openai/v1/chat/completions
LLM_MODEL=llama-3.3-70b-versatile
LLM_API_KEY=gsk_YOUR_GROQ_API_KEY_HERE

# Option 2: Use Ollama (Local LLM - Good for development)
# LLM_API_URL=http://localhost:11434/api/generate
# LLM_MODEL=llama2
# LLM_API_KEY=

# Option 3: Use OpenAI API
# LLM_API_URL=https://api.openai.com/v1/chat/completions
# LLM_MODEL=gpt-3.5-turbo
# LLM_API_KEY=sk-your-openai-api-key-here

# Option 4: Use LM Studio (Local OpenAI-compatible)
# LLM_API_URL=http://localhost:1234/v1/chat/completions
# LLM_MODEL=local-model-name
# LLM_API_KEY=

# Rate Limiting Configuration (optional)
# RATELIMIT_STORAGE_URI=memory://  # Use 'redis://localhost:6379' for production
# RATELIMIT_AI_GENERATION=5 per minute  # AI generation endpoints
# RATELIMIT_JOB_SEARCH=30 per minute  # Job search endpoints
# RATELIMIT_CAREER_RECOMMEND=10 per minute  # Career recommendation endpoints

# Caching Configuration (optional)
# CACHE_TYPE=simple  # Options: simple, redis, filesystem
# CACHE_DEFAULT_TIMEOUT=300  # 5 minutes in seconds
# CACHE_REDIS_URL=redis://localhost:6379/0  # Required if CACHE_TYPE=redis

# Instructions:
# 1. Copy this file to .env: cp .env.example .env
# 2. Replace YOUR_PASSWORD_HERE with your actual PostgreSQL password
# 3. Change JWT_SECRET_KEY to a random secure string for production
# 4. Configure LLM settings based on your choice:
#    - For Groq: API key already configured (very fast inference)
#    - For Ollama: Install from https://ollama.ai and run "ollama pull llama2"
#    - For OpenAI: Get API key from https://platform.openai.com/api-keys
#    - For LM Studio: Download from https://lmstudio.ai and start local server
# 5. For production, configure Redis for rate limiting and caching (see lines above)
